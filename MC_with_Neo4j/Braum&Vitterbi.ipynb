{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Model (HMM) with Neo4j Integration\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "\n",
    "- **Initialize an HMM and store its structure in Neo4j**\n",
    "- **Generate observations** from the model\n",
    "- **Infer the most likely hidden state sequence using the Viterbi algorithm**\n",
    "- **Train the model parameters using the Baum-Welch algorithm**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Connect to Neo4j\n",
    "\n",
    "In this step, we import the required libraries and establish a connection to the Neo4j graph database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from py2neo import Graph   # For interacting with the Neo4j graph database\n",
    "import numpy as np        # For numerical operations and matrix calculations\n",
    "import random             # For generating random choices\n",
    "\n",
    "# Connect to the Neo4j database with the provided credentials\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the Hidden Markov Model (HMM) Class\n",
    "\n",
    "The `HiddenMarkovModel` class initializes the HMM parameters and stores its structure (hidden states, observable states, transitions, and emissions) in Neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenMarkovModel:\n",
    "    def __init__(self, graph, hidden_states, observable_states, transition_matrix, emission_matrix, initial_distribution):\n",
    "        \"\"\"\n",
    "        Initializes a Hidden Markov Model (HMM) and stores it in a Neo4j database.\n",
    "        \n",
    "        Parameters:\n",
    "          - graph: The Neo4j database connection.\n",
    "          - hidden_states: List of hidden state labels.\n",
    "          - observable_states: List of observable state labels.\n",
    "          - transition_matrix: Matrix representing transition probabilities between hidden states.\n",
    "          - emission_matrix: Matrix representing emission probabilities from hidden to observable states.\n",
    "          - initial_distribution: Initial probability distribution for the hidden states.\n",
    "        \"\"\"\n",
    "        self.hidden_states = hidden_states\n",
    "        self.observable_states = observable_states\n",
    "        self.transition_matrix = np.array(transition_matrix)\n",
    "        self.emission_matrix = np.array(emission_matrix)\n",
    "        self.initial_distribution = np.array(initial_distribution)\n",
    "        self.graph = graph\n",
    "        # Store the HMM in Neo4j (clearing any existing data)\n",
    "        self.create_neo4j_hmm(clear=True, model_name=\"TrueModel\")\n",
    "    \n",
    "    def create_neo4j_hmm(self, clear=False, model_name=\"OriginalModel\"):\n",
    "        \"\"\"\n",
    "        Creates the HMM structure in the Neo4j database.\n",
    "        \n",
    "        It does the following:\n",
    "          1. Optionally clears the existing database nodes.\n",
    "          2. Creates nodes for hidden states with their initial probabilities.\n",
    "          3. Creates nodes for observable states.\n",
    "          4. Creates relationships for state transitions and emissions.\n",
    "        \n",
    "        Parameters:\n",
    "          - clear: If True, deletes existing nodes.\n",
    "          - model_name: Tag to label this version of the model in Neo4j.\n",
    "        \"\"\"\n",
    "        print(f\"Storing HMM in Neo4j as {model_name}...\")\n",
    "        if clear:\n",
    "            self.graph.run(\"MATCH (n) DETACH DELETE n\")\n",
    "\n",
    "        # Define node and edge labels based on the model name\n",
    "        node_type = \"HiddenState\" if model_name == \"TrueModel\" else f\"{model_name}Node\"\n",
    "        edge_type = \"TRANSITION\" if model_name == \"TrueModel\" else f\"{model_name}Connection\"\n",
    "\n",
    "        # Create hidden state nodes with initial probabilities\n",
    "        for state, prob in zip(self.hidden_states, self.initial_distribution):\n",
    "            self.graph.run(f\"\"\"\n",
    "                CREATE (:{node_type} {{name: $name, initial_prob: $prob, model: $model}})\n",
    "            \"\"\", parameters={\"name\": state, \"prob\": float(prob), \"model\": model_name})\n",
    "\n",
    "        # Create observable state nodes\n",
    "        for state in self.observable_states:\n",
    "            self.graph.run(f\"\"\"\n",
    "                CREATE (:ObservableState {{name: $name, model: $model}})\n",
    "            \"\"\", parameters={\"name\": state, \"model\": model_name})\n",
    "\n",
    "        # Create transition relationships between hidden states\n",
    "        for i, from_state in enumerate(self.hidden_states):\n",
    "            for j, to_state in enumerate(self.hidden_states):\n",
    "                prob = float(self.transition_matrix[i][j])\n",
    "                if prob > 0:\n",
    "                    self.graph.run(f\"\"\"\n",
    "                        MATCH (a:{node_type} {{name: $from, model: $model}}), (b:{node_type} {{name: $to, model: $model}})\n",
    "                        CREATE (a)-[:{edge_type} {{probability: $prob}}]->(b)\n",
    "                    \"\"\", parameters={\"from\": from_state, \"to\": to_state, \"prob\": prob, \"model\": model_name})\n",
    "        \n",
    "        # Create emission relationships from hidden states to observable states\n",
    "        for i, from_state in enumerate(self.hidden_states):\n",
    "            for j, obs in enumerate(self.observable_states):\n",
    "                prob = float(self.emission_matrix[i][j])\n",
    "                if prob > 0:\n",
    "                    self.graph.run(f\"\"\"\n",
    "                        MATCH (h:{node_type} {{name: $from, model: $model}}), (o:ObservableState {{name: $to, model: $model}})\n",
    "                        CREATE (h)-[:EMITS {{probability: $prob}}]->(o)\n",
    "                    \"\"\", parameters={\"from\": from_state, \"to\": obs, \"prob\": prob, \"model\": model_name})\n",
    "    \n",
    "    def generate_observations(self, n=10):\n",
    "        \"\"\"\n",
    "        Generates a sequence of observable states based on the HMM.\n",
    "        \n",
    "        The process:\n",
    "          1. Choose an initial hidden state based on the initial distribution.\n",
    "          2. For each step:\n",
    "             - Generate an observable state using the emission probabilities.\n",
    "             - Transition to the next hidden state based on the transition probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "          - n: Number of observations to generate.\n",
    "        \n",
    "        Returns:\n",
    "          - observations: List of generated observable states.\n",
    "          - order_hidden_states: Corresponding sequence of hidden states.\n",
    "        \"\"\"\n",
    "        observations = []\n",
    "        order_hidden_states = []\n",
    "        # Select the initial hidden state\n",
    "        state = np.random.choice(self.hidden_states, p=self.initial_distribution)\n",
    "        for _ in range(n):\n",
    "            # Generate an observable state from the current hidden state's emission distribution\n",
    "            obs = np.random.choice(self.observable_states, p=self.emission_matrix[self.hidden_states.index(state)])\n",
    "            observations.append(obs)\n",
    "            order_hidden_states.append(state)\n",
    "            # Transition to the next hidden state based on transition probabilities\n",
    "            state = np.random.choice(self.hidden_states, p=self.transition_matrix[self.hidden_states.index(state)])\n",
    "        return observations, order_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Implement the Viterbi Algorithm\n",
    "\n",
    "The `Viterbi` class uses dynamic programming to infer the most likely sequence of hidden states given a series of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Viterbi:\n",
    "    def __init__(self, hmm):\n",
    "        self.hmm = hmm\n",
    "    \n",
    "    def run(self, observations):\n",
    "        \"\"\"\n",
    "        Applies the Viterbi algorithm to find the most likely hidden state sequence.\n",
    "        \n",
    "        Steps:\n",
    "          1. **Initialization:** Calculate the probability for each hidden state given the first observation.\n",
    "          2. **Recursion:** For each subsequent observation, update the probabilities by considering the previous state probabilities,\n",
    "             transition probabilities, and the emission probability of the current observation.\n",
    "          3. **Backtracking:** Trace back the path with the highest probability to determine the sequence of hidden states.\n",
    "        \n",
    "        Parameters:\n",
    "          - observations: List of observable states.\n",
    "        \n",
    "        Returns:\n",
    "          - most_likely_states: The inferred sequence of hidden states.\n",
    "        \"\"\"\n",
    "        n_states = len(self.hmm.hidden_states)\n",
    "        n_obs = len(observations)\n",
    "        # Create a table to store the probability of the most likely path ending in each state at each time\n",
    "        viterbi_table = np.zeros((n_states, n_obs))\n",
    "        # Create a table to store the backpointers for the optimal path\n",
    "        backpointer = np.zeros((n_states, n_obs), dtype=int)\n",
    "\n",
    "        # Initialization for the first observation\n",
    "        for s in range(n_states):\n",
    "            viterbi_table[s, 0] = (self.hmm.initial_distribution[s] *\n",
    "                                   self.hmm.emission_matrix[s, self.hmm.observable_states.index(observations[0])])\n",
    "\n",
    "        # Recursion: Fill the viterbi table for each observation from time 1 onward\n",
    "        for t in range(1, n_obs):\n",
    "            for s in range(n_states):\n",
    "                max_prob, max_state = max(\n",
    "                    (\n",
    "                        viterbi_table[s_prev, t - 1] *\n",
    "                        self.hmm.transition_matrix[s_prev, s] *\n",
    "                        self.hmm.emission_matrix[s, self.hmm.observable_states.index(observations[t])],\n",
    "                        s_prev\n",
    "                    )\n",
    "                    for s_prev in range(n_states)\n",
    "                )\n",
    "                viterbi_table[s, t] = max_prob\n",
    "                backpointer[s, t] = max_state\n",
    "\n",
    "        # Backtracking: Find the most likely path by tracing back the pointers\n",
    "        best_last_state = np.argmax(viterbi_table[:, -1])\n",
    "        best_path = [best_last_state]\n",
    "        for t in range(n_obs - 1, 0, -1):\n",
    "            best_last_state = backpointer[best_last_state, t]\n",
    "            best_path.insert(0, best_last_state)\n",
    "        \n",
    "        # Convert state indices to actual state names\n",
    "        most_likely_states = [self.hmm.hidden_states[i] for i in best_path]\n",
    "        return most_likely_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configure the HMM for a Specific Example\n",
    "\n",
    "In this example, we define a simple \"Paper Bag\" problem:\n",
    "\n",
    "- **Hidden States:** Represent two bags (`A` and `B`).\n",
    "- **Observable States:** Represent chips (`j` and `k`).\n",
    "- **Transition Matrix:** Specifies the probability of moving between bags.\n",
    "- **Emission Matrix:** Specifies the probability of drawing a chip from each bag.\n",
    "- **Initial Distribution:** Always starts in Bag `A`.\n",
    "\n",
    "We then generate a sequence of observations and apply the Viterbi algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing HMM in Neo4j as TrueModel...\n",
      "\n",
      "=== Observations ===\n",
      "['j', 'k', 'k', 'j', 'k', 'k', 'k', 'k', 'k', 'k']\n",
      "\n",
      "=== True Order ===\n",
      "['A', 'A', 'B', 'A', 'B', 'A', 'B', 'B', 'A', 'A']\n",
      "\n",
      "=== Viterbi Path ===\n",
      "['A', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A']\n"
     ]
    }
   ],
   "source": [
    "# Define HMM parameters for the Paper Bag problem\n",
    "hidden_states_paperbag = ['A', 'B']          # Hidden states (bags)\n",
    "observable_states_paperbag = ['j', 'k']        # Observable states (chips)\n",
    "\n",
    "# Transition probabilities (Bag -> Bag)\n",
    "transition_matrix_paperbag = [\n",
    "    [0.40, 0.60],  # In Bag A: 40% chance to remain, 60% chance to switch to Bag B\n",
    "    [0.80, 0.20]   # In Bag B: 80% chance to remain, 20% chance to switch to Bag A\n",
    "]\n",
    "\n",
    "# Emission probabilities (Bag -> Chip)\n",
    "emission_matrix_paperbag = [\n",
    "    [2/5, 3/5],  # From Bag A: 40% chance for chip 'j', 60% for chip 'k'\n",
    "    [1/5, 4/5]   # From Bag B: 20% chance for chip 'j', 80% for chip 'k'\n",
    "]\n",
    "\n",
    "# Initial probability distribution (always start in Bag A)\n",
    "initial_distribution_paperbag = [1.0, 0.0]\n",
    "\n",
    "# Reconnect to Neo4j (if needed) and initialize the HMM\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "hmm = HiddenMarkovModel(graph, hidden_states_paperbag, observable_states_paperbag,\n",
    "                        transition_matrix_paperbag, emission_matrix_paperbag, initial_distribution_paperbag)\n",
    "\n",
    "# Generate a sequence of observations along with the true hidden state order\n",
    "observations, true_order = hmm.generate_observations(10)\n",
    "print(\"\\n=== Observations ===\")\n",
    "print(observations)\n",
    "\n",
    "print(\"\\n=== True Order ===\")\n",
    "print(true_order)\n",
    "\n",
    "# Run the Viterbi algorithm to infer the hidden state sequence from observations\n",
    "viterbi_model = Viterbi(hmm)\n",
    "viterbi_path = viterbi_model.run(observations)\n",
    "print(\"\\n=== Viterbi Path ===\")\n",
    "print(viterbi_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6d64c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of errors: 3.3\n",
      "Minimum number of errors: 1\n",
      "Maximum number of errors: 7\n",
      "Standard deviation of errors: 1.9000000000000001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define HMM parameters for the Paper Bag problem\n",
    "hidden_states = ['A', 'B']          # Hidden states (bags)\n",
    "observable_states = ['j', 'k']      # Observable states (chips)\n",
    "\n",
    "# Transition probabilities (Bag -> Bag)\n",
    "transition_matrix = np.array([\n",
    "    [0.40, 0.60],  # In Bag A: 40% chance to remain, 60% chance to switch to Bag B\n",
    "    [0.80, 0.20]   # In Bag B: 80% chance to remain, 20% chance to switch to Bag A\n",
    "])\n",
    "\n",
    "# Emission probabilities (Bag -> Chip)\n",
    "emission_matrix = np.array([\n",
    "    [2/5, 3/5],  # From Bag A: 40% chance for chip 'j', 60% for chip 'k'\n",
    "    [1/5, 4/5]   # From Bag B: 20% chance for chip 'j', 80% for chip 'k'\n",
    "])\n",
    "\n",
    "# Initial probability distribution (always start in Bag A)\n",
    "initial_distribution = np.array([1.0, 0.0])\n",
    "\n",
    "# Define number of runs and sequence length\n",
    "num_runs = 10\n",
    "sequence_length = 10\n",
    "\n",
    "# Function to generate an observation sequence and true states\n",
    "def generate_observations():\n",
    "    true_states = []\n",
    "    observations = []\n",
    "\n",
    "    # Start in Bag A\n",
    "    current_state = 0  # 'A' corresponds to index 0, 'B' to index 1\n",
    "\n",
    "    for _ in range(sequence_length):\n",
    "        true_states.append(hidden_states[current_state])\n",
    "        # Generate observation based on emission probabilities\n",
    "        observation = np.random.choice(observable_states, p=emission_matrix[current_state])\n",
    "        observations.append(observation)\n",
    "        # Transition to next state based on transition probabilities\n",
    "        current_state = np.random.choice([0, 1], p=transition_matrix[current_state])\n",
    "\n",
    "    return observations, true_states\n",
    "\n",
    "# Viterbi Algorithm Implementation\n",
    "def viterbi(observations):\n",
    "    num_states = len(hidden_states)\n",
    "    num_obs = len(observations)\n",
    "    \n",
    "    # Viterbi path probabilities and backpointers\n",
    "    viterbi_probs = np.zeros((num_states, num_obs))\n",
    "    backpointers = np.zeros((num_states, num_obs), dtype=int)\n",
    "\n",
    "    # Initialize with initial distribution\n",
    "    obs_index = observable_states.index(observations[0])\n",
    "    viterbi_probs[:, 0] = initial_distribution * emission_matrix[:, obs_index]\n",
    "\n",
    "    # Dynamic programming step\n",
    "    for t in range(1, num_obs):\n",
    "        obs_index = observable_states.index(observations[t])\n",
    "        for s in range(num_states):\n",
    "            prob_transition = viterbi_probs[:, t - 1] * transition_matrix[:, s]\n",
    "            best_prev_state = np.argmax(prob_transition)\n",
    "            viterbi_probs[s, t] = prob_transition[best_prev_state] * emission_matrix[s, obs_index]\n",
    "            backpointers[s, t] = best_prev_state\n",
    "\n",
    "    # Backtrack to get the most probable state sequence\n",
    "    best_final_state = np.argmax(viterbi_probs[:, -1])\n",
    "    best_path = [best_final_state]\n",
    "\n",
    "    for t in range(num_obs - 1, 0, -1):\n",
    "        best_final_state = backpointers[best_final_state, t]\n",
    "        best_path.insert(0, best_final_state)\n",
    "\n",
    "    # Convert index path to hidden state labels\n",
    "    inferred_states = [hidden_states[i] for i in best_path]\n",
    "    return inferred_states\n",
    "\n",
    "# Run the experiment 10 times and compute statistics\n",
    "total_errors = 0\n",
    "all_errors = []\n",
    "\n",
    "for _ in range(num_runs):\n",
    "    observations, true_states = generate_observations()\n",
    "    inferred_states = viterbi(observations)\n",
    "    errors = sum(1 for true, inferred in zip(true_states, inferred_states) if true != inferred)\n",
    "    total_errors += errors\n",
    "    all_errors.append(errors)\n",
    "\n",
    "average_errors = total_errors / num_runs\n",
    "min_errors = min(all_errors)\n",
    "max_errors = max(all_errors)\n",
    "std_dev_errors = np.std(all_errors)\n",
    "\n",
    "print(f'Average number of errors: {average_errors}')\n",
    "print(f'Minimum number of errors: {min_errors}')\n",
    "print(f'Maximum number of errors: {max_errors}')\n",
    "print(f'Standard deviation of errors: {std_dev_errors}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Implement the Baum-Welch Algorithm for Parameter Training\n",
    "\n",
    "The `BaumWelch` class applies the Expectation-Maximization (EM) technique to update the HMM parameters based on observed data.\n",
    "\n",
    "### Key Steps in Baum-Welch:\n",
    "1. **Forward Pass (Alpha):** Calculate the probability of partial observations given the model.\n",
    "2. **Backward Pass (Beta):** Calculate the probability of future observations given the current state.\n",
    "3. **Expectation (Gamma and Xi):**\n",
    "   - **Gamma:** Probability of being in a state at a certain time.\n",
    "   - **Xi:** Probability of transitioning between states at consecutive times.\n",
    "4. **Maximization:** Update the initial state distribution, transition matrix, and emission matrix.\n",
    "5. **Neo4j Update:** Save the updated model in Neo4j under a new model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BaumWelch:\n",
    "    def __init__(self, hmm):\n",
    "        self.hmm = hmm\n",
    "    \n",
    "    def run(self, observations, n_iterations=100):\n",
    "        \"\"\"\n",
    "        Runs the Baum-Welch algorithm to train the HMM parameters.\n",
    "        \n",
    "        Parameters:\n",
    "          - observations: List of observable states.\n",
    "          - n_iterations: Number of iterations to run the EM algorithm.\n",
    "        \n",
    "        Returns:\n",
    "          - Updated transition and emission matrices.\n",
    "        \"\"\"\n",
    "        n_states = len(self.hmm.hidden_states)\n",
    "        n_obs = len(observations)\n",
    "        obs_indices = [self.hmm.observable_states.index(obs) for obs in observations]\n",
    "\n",
    "        for _ in range(n_iterations):\n",
    "            alpha = np.zeros((n_states, n_obs))\n",
    "            beta = np.zeros((n_states, n_obs))\n",
    "            gamma = np.zeros((n_states, n_obs))\n",
    "            xi = np.zeros((n_states, n_states, n_obs - 1))\n",
    "            \n",
    "            # Forward Pass\n",
    "            alpha[:, 0] = self.hmm.initial_distribution * self.hmm.emission_matrix[:, obs_indices[0]]\n",
    "            for t in range(1, n_obs):\n",
    "                alpha[:, t] = (alpha[:, t - 1] @ self.hmm.transition_matrix) * self.hmm.emission_matrix[:, obs_indices[t]]\n",
    "            \n",
    "            # Backward Pass\n",
    "            beta[:, -1] = 1\n",
    "            for t in range(n_obs - 2, -1, -1):\n",
    "                beta[:, t] = self.hmm.transition_matrix @ (self.hmm.emission_matrix[:, obs_indices[t + 1]] * beta[:, t + 1])\n",
    "            \n",
    "            # Compute Gamma\n",
    "            gamma = (alpha * beta) / np.sum(alpha * beta, axis=0, keepdims=True)\n",
    "            \n",
    "            # Compute Xi\n",
    "            for t in range(n_obs - 1):\n",
    "                denominator = np.sum(alpha[:, t] * self.hmm.transition_matrix * self.hmm.emission_matrix[:, obs_indices[t + 1]] * beta[:, t + 1])\n",
    "                if denominator == 0:\n",
    "                    denominator = 1e-10  # Avoid division by zero\n",
    "                xi[:, :, t] = (alpha[:, t, None] * self.hmm.transition_matrix * self.hmm.emission_matrix[:, obs_indices[t + 1]] * beta[:, t + 1]) / denominator\n",
    "            \n",
    "            # Update Initial Distribution\n",
    "            self.hmm.initial_distribution = gamma[:, 0]\n",
    "            \n",
    "            # Update Transition Matrix\n",
    "            denom = np.sum(gamma[:, :-1], axis=1, keepdims=True)\n",
    "            denom[denom == 0] = 1e-10  # Avoid division by zero\n",
    "            self.hmm.transition_matrix = np.sum(xi, axis=2) / denom\n",
    "            \n",
    "            # Ensure row normalization\n",
    "            self.hmm.transition_matrix /= self.hmm.transition_matrix.sum(axis=1, keepdims=True)\n",
    "            \n",
    "            # Update Emission Matrix\n",
    "            for k in range(len(self.hmm.observable_states)):\n",
    "                mask = np.array(obs_indices) == k\n",
    "                denom = np.sum(gamma, axis=1, keepdims=True)\n",
    "                denom[denom == 0] = 1e-10  # Avoid division by zero\n",
    "                self.hmm.emission_matrix[:, k] = np.sum(gamma[:, mask], axis=1) / denom.squeeze()\n",
    "            \n",
    "            # Ensure emission matrix row normalization\n",
    "            self.hmm.emission_matrix /= self.hmm.emission_matrix.sum(axis=1, keepdims=True)\n",
    "            \n",
    "        # Store updated parameters in Neo4j\n",
    "        self.hmm.create_neo4j_hmm(clear=False, model_name=\"BaumWelch\")\n",
    "        \n",
    "        return self.hmm.transition_matrix, self.hmm.emission_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing HMM in Neo4j as TrueModel...\n",
      "Generated Observations: ['k', 'k', 'j', 'k', 'k', 'j', 'j', 'j', 'k', 'k', 'j', 'j', 'j', 'k', 'j', 'k', 'j', 'j', 'k', 'j', 'k', 'k', 'k', 'j', 'j', 'j', 'k', 'k', 'k', 'k', 'k', 'k', 'k', 'j', 'k', 'k', 'k', 'j', 'j', 'k', 'k', 'j', 'k', 'k', 'j', 'k', 'j', 'j', 'k', 'k', 'j', 'k', 'k', 'k', 'k', 'k', 'k', 'j', 'j', 'k', 'k', 'k', 'k', 'k', 'k', 'k', 'k', 'k', 'j', 'j', 'k', 'j', 'k', 'k', 'k', 'k', 'j', 'j', 'j', 'k', 'k', 'k', 'k', 'k', 'k', 'j', 'k', 'j', 'k', 'j', 'k', 'k', 'k', 'k', 'j', 'k', 'j', 'k', 'k', 'j']\n",
      "Storing HMM in Neo4j as BaumWelch...\n",
      "Initial Transition Matrix:\n",
      " [[0.4, 0.6], [0.8, 0.2]]\n",
      "Final Updated Transition Matrix:\n",
      " [[0.32353953 0.67646047]\n",
      " [0.80560111 0.19439889]]\n",
      "Initial Emission Matrix:\n",
      " [[0.4, 0.6], [0.2, 0.8]]\n",
      "Final Updated Emission Matrix:\n",
      " [[0.3564481  0.6435519 ]\n",
      " [0.36428269 0.63571731]]\n"
     ]
    }
   ],
   "source": [
    "# Reinitialize HMM parameters for Baum-Welch training (same setup as before)\n",
    "hidden_states_paperbag = ['A', 'B']\n",
    "observable_states_paperbag = ['j', 'k']\n",
    "\n",
    "transition_matrix_paperbag = [\n",
    "    [0.40, 0.60],\n",
    "    [0.80, 0.20]\n",
    "]\n",
    "\n",
    "emission_matrix_paperbag = [\n",
    "    [2/5, 3/5],\n",
    "    [1/5, 4/5]\n",
    "]\n",
    "\n",
    "initial_distribution_paperbag = [1.0, 0.0]\n",
    "\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "hmm = HiddenMarkovModel(graph, hidden_states_paperbag, observable_states_paperbag,\n",
    "                        transition_matrix_paperbag, emission_matrix_paperbag, initial_distribution_paperbag)\n",
    "\n",
    "# Generate a longer sequence of observations for training the model\n",
    "observations, _ = hmm.generate_observations(n=100)\n",
    "print(\"Generated Observations:\", observations)\n",
    "\n",
    "# Train the HMM using the Baum-Welch algorithm\n",
    "baum_welch = BaumWelch(hmm)\n",
    "updated_transition_matrix, updated_emission_matrix = baum_welch.run(observations)\n",
    "print(\"Initial Transition Matrix:\\n\", transition_matrix_paperbag)\n",
    "print(\"Final Updated Transition Matrix:\\n\", updated_transition_matrix)\n",
    "print(\"Initial Emission Matrix:\\n\", emission_matrix_paperbag)\n",
    "print(\"Final Updated Emission Matrix:\\n\", updated_emission_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provided a detailed walkthrough of:\n",
    "\n",
    "- **Storing an HMM in Neo4j:** Creating nodes for hidden and observable states and linking them with transition and emission relationships.\n",
    "- **Generating Observations:** Simulating a series of observable outputs based on the HMM parameters.\n",
    "- **Inference with Viterbi:** Determining the most likely sequence of hidden states given a series of observations.\n",
    "- **Training with Baum-Welch:** Refining the model parameters using the EM approach and updating the model in Neo4j.\n",
    "\n",
    "Feel free to adjust or extend this notebook for your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaf320a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
